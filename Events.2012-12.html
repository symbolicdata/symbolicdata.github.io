<p><a href="Main_Page" title="wikilink">Main_Page</a> &gt; <a href="Events" title="wikilink">Events</a></p>
<h3 id="december-2012---workshop-on-symbolicdata-design-in-leipzig.">13.-14. December 2012 - Workshop on SymbolicData Design in Leipzig.</h3>
<p>Within the <a href="Projects.EScience" title="wikilink">E-Science Benchmarking Project</a> we invited for a Workshop and Hackathon to discuss and promote different aspects of the SymbolicData Project. The Workshop took place at <a href="http://www.htwk-leipzig.de">HTWK</a> - Hochschule für Technik, Wirtschaft und Kultur Leipzig.</p>
<p><a href="http://portal.imn.htwk-leipzig.de/events/workshop-on-symbolicdata-design">Announcement</a> of the meeting by HTWK.</p>
<p>Participants:</p>
<ul>
<li>Hans-Gert Gräbe, Andreas Nareike (Uni Leipzig)</li>
<li>Johannes Waldmann (HTWK Leipzig)</li>
<li>Viktor Levandovskyy, Albert Heinle, Benjamin Schnitzler (RTWH Aachen)</li>
<li>Satya S. Samal (Uni Bonn)</li>
</ul>
<h3 id="program">Program</h3>
<p><strong>Thursday, 13.12.</strong></p>
<ul>
<li>13:00 Registration, Come Together</li>
<li>14:00 Opening, Presentation and Discussion of the SymbolicData Project overall structure</li>
<li>16:00 Presentation and Discussion of the Free Algebra Data Part</li>
<li>18:00 Presentation and Discussion about Scripts and Tools</li>
<li>19:30 Kneipe</li>
</ul>
<p><strong>Friday, 14.12.</strong></p>
<ul>
<li>9:00 Plenary Discussion about the future infrastructure of SymbolicData</li>
<li>11:00 Presentation on the <a href="http://pocab.cg.cs.uni-bonn.de">PoCaB</a> Project</li>
<li>13:30 Benchmarks and Competitions in Termination, SAT and SMT (J.Waldmann, HTWK Leipzig) (<a href="http://www.imn.htwk-leipzig.de/~waldmann/talk/12/sym/main.pdf">Slides</a>)</li>
<li>15:00 Connecting large scientific Projects as <a href="http://www.sagemath.org/">SageMath</a> or <a href="http://pocab.cg.cs.uni-bonn.de">PoCaB</a> or <a href="http://www.starexec.org/starexec/public/about.jsp">StarExec</a></li>
<li>17:30 Kneipe</li>
</ul>
<h3 id="a-short-summary">A Short Summary</h3>
<p>We had two days of intense discussions about the goals, philosophy, subprojects, links etc. of the SymbolicData Project.</p>
<p>First we discussed the current state of the project. <em>Hans-Gert Gräbe</em> explained in detail the work done so far towards a redesign of the Data collection according to Linked Open Data standards. Within this refactoring process we distinguish more clearly between <em>Data</em> (called <em>XMLResources</em>, although it turned out during the discussion that one should not restrict the support of syntactical expressiveness to XML only) and <em>Metadata</em> (called <em>RDFResources</em>, since interlinking of metadata is nowadays best supported by the RDF based <a href="http://en.wikipedia.org/wiki/Semantic_Web_Stack">Semantic Web Stack</a>). Such a distinction allows to express more clearly another point: <em>Data</em> and it semantic meaning are managed <em>within</em> different Computer Algebra Communities, <em>Metadata</em> are required for <em>Cross Community Communication</em> purposes. The main future focus of SymbolicData will be on the needs of such a Cross Community Communication between different Computer Algebra Communities.</p>
<p><em>Albert Heinle</em> presented the <a href="SDEval" title="wikilink">SDEval</a> framework. It grew up from the profiling and testing needs of the <a href="FreeAlgebras" title="wikilink">Free Algebra</a> community, but is generic enough to serve as a best practice how to organize automated set up, run, evaluation and comparison of dedicated computational tasks on a large amount of data. The framework is written in <em>python</em> and has a (not yet explored) conceptual overlap with the <a href="http://en.wikipedia.org/wiki/JUnit">JUnit framework</a> best practices. It heavily uses UNIX process management facilities to flexibly define and set up computational environments with dedicated characteristics and can be reused for a wide range of computational tasks with different CA software. SDEval continues the SymbolicData efforts to establish standards how to set up environments for a <em><a href="http://en.wikipedia.org/wiki/Cross-cutting_concern">cross cutting concern</a></em> as testing and benchmarking of CA software on a larger collection of given data.</p>
<p><em>Satya Samal</em> presented the PoCaB Project - Plattform of Chemical and Biological Analysis Using Computer Algebra Methods - and explained in detail structural approaches within the PoCaB Databases and how data are generated within the PoCaB framework. PoCaB mainly addresses topics around categorization of differential equation systems in mass action and non-mass action kinetics in chemical systems coming from a biological background. PoCaB is interlinked with different communities within CA (Polynomial Systems Solving and the polymake community) and also beyond. In particular, it heavily exploits <a href="http://www.ebi.ac.uk/biomodels-main/">biological databases</a> (BioModel Database, KEGG Database) that come with their own language <a href="http://sbml.org">SMBL</a> and experiences how to express semantical aspects in a computer readable way. This example shows very clearly that CA Communities are not interested in advice from outside how to reinvent wheels properly running for a long time <em>within</em> the community but acknowledge support and advice how to organise inter community communication properly and more smoothly in a world of evolving standards. It requires more effort to understand what needs of PoCaB can be addressed by and in cooperation with the SymbolicData Project and how to do that.</p>
<p><em>Johannes Waldmann</em> gave a talk about Benchmarks and Competitions in Theoretical Computer Science presenting best practices of three TCS Communities: Termination, SAT and SMT. For Termination he explained TPBD - the <a href="http://termination-portal.org/wiki/TPDB">Termination Problems Data Base</a> - and their way of benchmarking: They regulary organize Termination Competitions on previously agreed data from different problem categories in a similar way as the &quot;Formel I&quot; car race is organized: Upload tools to a single dedicated server that runs all tolls on all problems and presents the results in aggregated form an a web page. Usually such a competition runs accompanying the annual large conference in the field. All communities have their own (intracommunity) infrastructure - workshops, mailing lists, wiki (to carry forward a &quot;common story&quot;) - and domain specific</p>
<ul>
<li>input syntax and semantics specification,</li>
<li>standards for what is an acceptable proof trace (none - informal - verifyable),</li>
<li>methods for selecting competition problems,</li>
<li>algorithms for scoring results.</li>
</ul>
<p>Joe Waldmann pointed our attention to the <a href="http://www.starexec.org/starexec/public/about.jsp">StarExec</a> project</p>
<p>Starexec is a cross community logic solving service under development at the University of Iowa ... Our goal is to provide a shared logic solving infrastructure to researchers to manage benchmark libraries, community membership, and provide solver execution on a large cluster and facilitate translation between logics. ... We are funded by a $1.85 million USD grant from the National Science Foundation ...</p>
<p>The StarExec Project has the goal to provide a domain-agnostic execution platform (software and hardware) for running competitions in computational logics and developed some meta-model of competitions that covers standards for benchmarks, tools and results.</p>
<p>Let me summarize the results of the workshop for the future of SymbolicData:</p>
<p>First we decided to refocus the SymbolicData Project to address needs of <em>communities</em> within Symbolic Computation to profile, test and benchmark implementations. There is a commonly complained misrecognition of these efforts because such contributions are not in the focus of reputational processes of the respective communities and are rarely proper acknowledged. This is mainly due to the specifics in that area, and SymbolicData will address, collect, offer, ... standards and best practices to change that <em>across</em> the different CA communities.</p>
<p>SymbolicData (v.1 and v.2) had its origin within the Polynomial Systems Community, so such a refocussing has to be processed also as a reorganization of data for SymbolicData v.3. A list of communities (at the moment called &quot;areas&quot;) with benchmarking activities addressed by SymbolicData will be maintained on the <a href="Main_Page" title="wikilink">wiki main page</a>, and there is a process of standardized presentation of community aims on the way.</p>
<p>For the future there should be a better interlinking between (intracommunity) sources, resources and communication structures <em>within</em> such a community and SymbolicData. A new use case to really understand the problems with such interlinking is given with <a href="http://pocab.cg.cs.uni-bonn.de">PoCaB</a>. This will be a great step towards Linked Data. Other use cases are given by the Databases of the SPP1489.</p>
<p><a href="SDEval" title="wikilink">SDEval</a> - a common benchmarking compute framework written in python and (more or less) ready to use also beyond the <a href="FreeAlgebras" title="wikilink">Free Algebra</a> community - represents best practice of running dedicated computational tasks on a large amount of given data. This code is (almost) available from the SymbolicData Public Repository.</p>
<p>In the near future we focus on consolidating SymbolicData and releasing a stable v.3. As a first step we moved to git and operate a <a href="https://github.com/symbolicdata">public Repo</a> at github. See <a href="Using.Git" title="wikilink">Using.Git</a> for more information about the organization of the public Repo and our Git Development Process.</p>
<p>There is a <a href="http://en.wikipedia.org/wiki/SPARQL">Sparql endpoint</a> for SymbolicData available at <a href="http://symbolicdata.ontowiki.net">http://symbolicdata.ontowiki.net</a> that serves the latest RDFData.</p>
<p>In the second half of July there will be another workshop in Leipzig to &quot;count the chickens we hatched&quot; and to estimate what left after the E-Science project will end at Aug 31, 2013.</p>
